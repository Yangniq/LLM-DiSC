# from test_code import log_main
from env import INFO
import time
import numpy as np
import llm
from cbf import CBFController
from log_fig import figure, ctrl_figure


def euclidean_distance(p1, p2):
    return np.linalg.norm(np.array(p1) - np.array(p2))


def update(info: INFO, run_directory):
    model = info.dynamics_model
    CBF = CBFController(model,info.agent_radius,info.zeta,info.eta)

    def reset_state():
        states = np.array(info.begin_state, dtype=float)
        all_ctrl = np.array(info.begin_ctrl, dtype=float)
        statesInfo = [[list(states[j])] for j in range(info.agent_num)]
        posInfo = [[list(states[j][:2])] for j in range(info.agent_num)]
        ctrlInfo = [[] for _ in range(info.agent_num)]
        newCInfo = [[] for _ in range(info.agent_num)]
        cbf_obs = [[] for _ in range(info.agent_num)]
        cbf_agent = [[] for _ in range(info.agent_num)]
        reached_targets = [False] * info.agent_num
        return states, all_ctrl, statesInfo, posInfo, ctrlInfo, newCInfo, cbf_obs, cbf_agent,reached_targets


    i = 0
    states, all_ctrl, statesInfo, posInfo, ctrlInfo, newCInfo, cbf_obs, cbf_agent,reached_targets = reset_state()
    iterations = 1

    while i <= info.maxStep and iterations <= info.iteration_time:
        dis = 0
        success = False
        # Update the status of each agent
        for j in range(info.agent_num):
            # Call the function generated by llm to obtain the control signal
            while True:
                try:
                    function = llm.load_function_from_file()
                    ctrl_U = function(j, states,all_ctrl, info.target_pos[j], info.poly_pos,
                                      info.agent_radius, statesInfo)
                    break
                except Exception as e:
                    print(f"Error while calling the function in update: {e}")

                    # Feed back the error information to the LLM
                    llm.conversation_history.append(
                        {"role": "user", "content": f"Error encountered: {e}"})
                    llm.log_interaction(run_directory, llm.conversation_history)
                    # Call the LLM again to generate new code
                    llm.main()

            constraint_obs_all, constraint_agent_all, G, H, _, _ = CBF.cbf_constraint(ctrl_U, j, all_ctrl, states, info.obs,
                                                                                info.agent_num, info.obs_num)
            cbf_obs[j].append(constraint_obs_all)
            cbf_agent[j].append(constraint_agent_all)
            # Determine whether the cbf constraint conditions are met
            flag_agent = all(x >= 0 for x in constraint_agent_all)
            flag_obs = all(x >= 0 for x in constraint_obs_all)
            cbf_obs_100 = cbf_obs[j][-50:]
            cbf_agent_100 = cbf_agent[j][-50:]
            if flag_obs and flag_agent:
                newU = ctrl_U
                all_ctrl[j] = newU
            else:
                result = CBF.correct_U( ctrl_U, G, H)
                if result["status"] == "error":
                    error_message = result["message"]
                    print(f"Error while calling the function 'correct_U' in update: {error_message}")
                    _, theta = ctrl_U
                    newU = [0, theta]
                    all_ctrl[j] = newU

                else:
                    newU = result["data"]
                    all_ctrl[j] = newU
            ctrlInfo[j].append(ctrl_U)
            newCInfo[j].append(newU)
            # Update the status of the agent
            states[j] = model.update_state(states[j], newU, info.dt)
            # Store location information
            statesInfo[j].append(states[j].tolist())
            posInfo[j].append(states[j][:2].tolist())
            # Calculate the distance between the agent and the target point to determine whether the task is completed
            dis_agent = np.linalg.norm(states[j][:2] - np.array(info.target_pos[j]))
            if not reached_targets[j]:
                if np.allclose(newU, [0, 0]) and dis_agent <= 2 * info.agent_radius:
                    reached_targets[j] = True
                # elif dis_agent <= info.agent_radius:
                #     reached_targets[j] = True
            dis += dis_agent
            elements_to_check = posInfo[j][-500:]
            # Calculate the Euclidean distance between the current position and the position 100 steps ago
            if len(elements_to_check) >= 500:
                dis_50_steps_ago = euclidean_distance(states[j][:2], elements_to_check[0])
            else:
                dis_50_steps_ago = 5
            if dis_agent > 2 and dis_50_steps_ago <= 0.5 and reached_targets[j]== False:
                print(f'The agent {j} stops moving')
                if iterations == 10:
                    dis = 3
                    iterations += 1
                    break
                else:
                    ctrl_signal = ctrlInfo[j][-100:]
                    state_info100 = statesInfo[j][-50:]
                    figure(posInfo, info, run_directory)
                    ctrl_figure(ctrlInfo, newCInfo, run_directory, info.agent_num)
                    # Feed back the information to the LLM
                    llm.conversation_history.append(
                        {"role": "user",
                         "content": fr"""
    To ensure the safety of the agents, we incorporate the control barrier function (CBF) into the current controller to verify whether the control signal maintains the agent within a safe region. The CBF is defined as $h(\boldsymbol{{x}}) = (p_x - p_{{x_o}})^2+(p_y- p_{{y_o}})^2 - R^2$. For obstacles, $[p_{{x_o}},p_{{y_o}}]$ is the closest point to the agent on the obstacle and $R$ is equal the agent's radius $r$. For other agents, $[p_{{x_o}},p_{{y_o}}]$ is the position of other agents and $R$ is equal to $2r$. The CBF $h(\boldsymbol{{x}})$ determines the safety set of the agent, denoted as $S:=\{{\boldsymbol{{x}}:h(\boldsymbol{{x}}) \geq0\}}$. To ensure that the system does not leave the safe set, the control input $\boldsymbol{{u}}$ should satisfy the CBF constraint $\dot{{h}}(\boldsymbol{{x}})+2h(\boldsymbol{{x}}) \geq 0$, i.e. $2(p_x - p_{{x_o}})v\cos(\theta)+2(p_y- p_{{y_o}})v\sin(\theta) +2h(\boldsymbol{{x}}) \geq 0$. When this inequality constraint $2(p_x - p_{{x_o}})v\cos(\theta)+2(p_y- p_{{y_o}})v\sin(\theta) +2h(\boldsymbol{{x}}) \geq 0$ approaches zero, it indicates that the agent is nearing the unsafe region, so the control input should be adjusted to increase the value of $2(p_x - p_{{x_o}})v\cos(\theta)+2(p_y- p_{{y_o}})v\sin(\theta) +2h(\boldsymbol{{x}}) \geq 0$.
    
    Under this controller, when the states of the {info.agent_num} agents are {states}, the agent with the state `{states[j]}` encounters difficulties in progressing toward its target. Based on experience, the agent is likely to face safety issues (such as collision with obstacles or other agents), become trapped in cyclical motion, or fall into local optima when the agent is far away from its target point. Please note that only a few common issues are listed here, and other unmentioned issues may also occur.
    
    The agent's state over the past 50 steps is as follows: {state_info100}. 
    
    The changes in the CBF constraint values over the past 50 state updates are as follows: the obstacle constraint values are {cbf_obs_100}, representing the values of $\dot{{h}}_{{obs1}} + 2h_{{obs1}} , \dot{{h}}_{{obs2}} +2h_{{obs2}} \dots$. 
    
    Please analyze the changes in the agent's states to identify potential issues it may encounter under the current controller. Make necessary improvements to enhance the controller's performance, enabling more **flexible** adjustments to the agent's orientation and speed when navigating around obstacles and other agents.
    
    If necessary, prioritizing obstacle avoidance and then moving toward the target point after reaching a safe area may be a viable solution. For example, when the distance from the current position to an obstacle is detected to be less than a safe threshold, calculate a safe direction away from the obstacle and adjust the target linear and angular velocities accordingly:
```
if distance_to_obstacle < safety_margin:
    # Calculate a safe direction away from the obstacle
    safe_direction = np.array([x - closest_point[0], y - closest_point[1]])
    safe_direction /= np.linalg.norm(safe_direction)
    v_target = 0.5 * v_max  # Reduce target velocity
    omega_target = np.arctan2(safe_direction[1], safe_direction[0]) - theta
    omega_target = (omega_target + np.pi) % (2 * np.pi) - np.pi
    v = v_target
    omega = omega_target
```
    **Be sure not to change the function name.**
    """})
                    llm.log_interaction(run_directory, llm.conversation_history)
                    llm.main()
                    i = 0
                    states, all_ctrl, statesInfo, posInfo, ctrlInfo, newCInfo, cbf_obs, cbf_agent,reached_targets = reset_state()
                    dis = 2.6
                    iterations += 1
                    break

        if all(reached_targets):
            print(f'dis: {dis}')
            success = True
            break
        elif i == info.maxStep:
            print('Reached the maximum number of steps.')
            if iterations == 10:
                iterations += 1
            else:
                figure(posInfo, info, run_directory)
                ctrl_figure(ctrlInfo, newCInfo, run_directory, info.agent_num)
                llm.conversation_history.append(
                    {"role": "user", "content": f"""When the maximum update steps are reached, the agents still have not arrived at the target points.                
    This may be due to one of the following reasons:
    1. When the agent reaches near the target point, it performs circular motion around the target point;
    2. The control signal exhibits periodic changes, causing the agent to move in a loop within a certain area; 
    3. The agents are not moving towards the target points (they are getting farther from the target point);
    4. The agent stops moving while still some distance away from the target point.
    Please check whether the controller could potentially cause these issues and make the necessary adjustments. Please ensure that the changes made are helpful and briefly explain the modifications.
    """})
                llm.log_interaction(run_directory, llm.conversation_history)
                llm.main()
                i = 0
                states, all_ctrl, statesInfo, posInfo, ctrlInfo, newCInfo, cbf_obs, cbf_agent,reached_targets = reset_state()
                iterations += 1
        else:
            i += 1

    return statesInfo, posInfo, ctrlInfo, newCInfo,success, iterations


if __name__ == "__main__":
    env_info = INFO()
    start_time = time.time()
    run_directory_info = llm.main()
    states_info, pos_info, ctrl_info, newC_info,flag, iterations = update(env_info, run_directory_info)
    end_time = time.time()
    llm.log_run_time(run_directory_info, end_time - start_time)
    figure(pos_info, env_info, run_directory_info)
    ctrl_figure(ctrl_info, newC_info, run_directory_info, env_info.agent_num)

